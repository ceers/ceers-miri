{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8fcb3c",
   "metadata": {},
   "source": [
    "# Reduce MIRI imaging data with JWST pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "import multiprocessing\n",
    "\n",
    "# Set the CRDS path and url\n",
    "# should modify the path according to your computer!!\n",
    "# This must be set before importing the pipeline!\n",
    "os.environ['CRDS_PATH']= '/Users/users/gyang/gyang/crds_cache/jwst_pub'\n",
    "os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds-pub.stsci.edu'\n",
    "\n",
    "import jwst\n",
    "from jwst import pipeline\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Image2Pipeline\n",
    "from jwst.pipeline import Image3Pipeline\n",
    "from jwst.associations import asn_from_list\n",
    "from jwst.associations.lib.rules_level3 import Asn_Lv3Image\n",
    "from jwst import datamodels\n",
    "from jwst.resample.resample_utils import build_driz_weight\n",
    "\n",
    "from astropy.table import Table\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates.sky_coordinate import SkyCoord\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "import astropy.units as u\n",
    "\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%run -i 'write_rms_map.ipynb'\n",
    "%run -i 'run_tkreg.ipynb'\n",
    "%run -i 'super_bg.ipynb'\n",
    "%run -i 'group_imgs.ipynb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c998779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neg_jump(frame1, frame2, frac_thresh=0.5):\n",
    "    # Get the shape of the framerix\n",
    "    Ny, Nx = frame1.shape\n",
    "    # Calculate sign from the difference between frame2 & frame1\n",
    "    sign_det_frame = ( (frame2-frame1)<0 )*1\n",
    "    # Calculate the fraction of negative difference in each row\n",
    "    neg_fracs = sign_det_frame.sum(axis=1) / Nx\n",
    "    # Select the jump rows \n",
    "    jump_row_idxs = np.where(neg_fracs>frac_thresh)[0]\n",
    "    \n",
    "    if len(jump_row_idxs)==0: \n",
    "        # If no jump\n",
    "        return None, None\n",
    "    elif jump_row_idxs[-1]-jump_row_idxs[0]+1 == len(jump_row_idxs):\n",
    "        # Check if the jump rows are together (expected)\n",
    "        # Here we mask expand an addtional row to be safe\n",
    "        lo_row_idx, up_row_idx = jump_row_idxs[0], jump_row_idxs[-1]\n",
    "        if lo_row_idx != 0: lo_row_idx -=1\n",
    "        if up_row_idx != Ny-1: up_row_idx +=1\n",
    "        return lo_row_idx, up_row_idx\n",
    "    else:\n",
    "        print(jump_row_idxs)\n",
    "        raise Exception('jump rows are not continuous?!')\n",
    "        \n",
    "def check_neg_jump(frame1, frame2, lo_row_idx, up_row_idx, frac_thresh=0.5):\n",
    "    # Calculate negative jump fraction\n",
    "    sign_det_frame = (frame2[lo_row_idx:up_row_idx+1]-frame1[lo_row_idx:up_row_idx+1] < 0) * 1\n",
    "    neg_frac = sign_det_frame.sum() / (sign_det_frame.shape[0]*sign_det_frame.shape[1])\n",
    "    # if satisfies the negative-jump criterion, return *False*\n",
    "    if neg_frac>frac_thresh: return False\n",
    "    else: return True\n",
    "\n",
    "def set_dq(dq, m, n, k, i, j):\n",
    "    # Make sure n>k\n",
    "    if n<k: n, k = k, n\n",
    "    \n",
    "    # Set to DO_NOT_USE if the groups are at the beginning\n",
    "    # of the integration \n",
    "    if k<=1: \n",
    "        # Set to DO_NOT_USE (Jane Morrison)\n",
    "        # Clean up the JUMP_DET\n",
    "        use = (dq[m,k:n+1,i:j+1]%8//4==1)\n",
    "        dq[m,k:n+1,i:j+1][use] -= 4\n",
    "        # Only need to reset for non-DO_NOT_USE pixels\n",
    "        use = dq[m,k:n+1,i:j+1] % 2 ==0\n",
    "        dq[m,k:n+1,i:j+1][use] += 1\n",
    "        print(\"  set to DO_NOT_USE\")\n",
    "    else:\n",
    "        # Only need to reset for non-jump pixels\n",
    "        use = dq[m,k:n+1,i:j+1] % 8 // 4 ==0\n",
    "        dq[m,k:n+1,i:j+1][use] += 4\n",
    "        print(\"  set to JUMP_DET\")\n",
    "        \n",
    "    print( '  %d pixels corrected' %len(np.where(use)[0]) )\n",
    "    return dq\n",
    "\n",
    "def validate(dq, m, n, i, j):\n",
    "    if m<0 or m>=dq.shape[0]: return False\n",
    "    if n<0 or n>=dq.shape[1]: return False\n",
    "    if i<0 or i>=dq.shape[2] or j<0 or j>=dq.shape[2] or i>j: return False\n",
    "    if (dq[m,n,i:j+1]%2==1).all(): return False\n",
    "    return True\n",
    "\n",
    "def correct_jump(datamodel):\n",
    "    data = datamodel.data\n",
    "    dq = datamodel.groupdq\n",
    "    # Get the number of integraions and frames (groups) \n",
    "    Ni, Nf, Ny, Nx = data.shape\n",
    "    # Iterate over each frame pair\n",
    "    for m in range(Ni):\n",
    "        for n in range(1, Nf):\n",
    "            # Check if the frame is good\n",
    "            if not validate(dq, m, n-1, 0, Ny-1) or not validate(dq, m, n, 0, Ny-1): continue\n",
    "            # Try to find rows with negative jump\n",
    "            # Note that we only search around n+-2\n",
    "            i, j = find_neg_jump(data[m, n-1], data[m, n])\n",
    "            if (i == None) or not validate(dq, m, n-1, i, j) or not validate(dq, m, n, i, j): continue\n",
    "            # Try to locate the corresponding positive jump\n",
    "            # and set the corrupted frames/rows to DO_NOT_USE\n",
    "            if   validate(dq, m, n-2, i, j) and check_neg_jump(data[m, n-2], data[m, n],  i, j):\n",
    "                k = n-1\n",
    "            elif validate(dq, m, n+1, i, j) and check_neg_jump(data[m, n-1], data[m, n+1], i, j):\n",
    "                k = n+1\n",
    "            else:\n",
    "                if   validate(dq, m, n-3, i, j):\n",
    "                    if check_neg_jump(data[m, n-3], data[m, n],  i, j): k=n-2\n",
    "                    else: k=n+2\n",
    "                elif validate(dq, m, n+2, i, j):\n",
    "                    if check_neg_jump(data[m, n-1], data[m, n+2], i, j): k=n+2\n",
    "                    else: k=n-2\n",
    "                else:\n",
    "                    raise Exception(\"sorry, can't locate positive jump due to incomplete data\")\n",
    "            # Set bad segements to \"DO_NOT_USE\"\n",
    "            print('ITG=%d, neg=%d, pos=%d, row=%d-%d' %(m+1, n+1, k+1, i+1, j+1))\n",
    "            dq = set_dq(dq, m, n, k, i, j)\n",
    "            \n",
    "    datamodel.groupdq = dq\n",
    "    return datamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5fce",
   "metadata": {},
   "source": [
    "# Show the jump effect\n",
    "%run -i '../env_set.ipynb'\n",
    "raw_data = fits.getdata('../data/raw/jw01345/mastDownload/JWST/jw01345001001_14101_00002_mirimage/jw01345001001_14101_00002_mirimage_uncal.fits')\n",
    "\n",
    "# Plot the ramp data of two pixels\n",
    "itg_idx = 4\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "ramp1 = raw_data[itg_idx, :, 850, 700]/1e4\n",
    "ramp2 = raw_data[itg_idx, :, 150, 700]/1e4\n",
    "group_id = np.arange(len(ramp1)) + 1\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(group_id, ramp1, 'ok')\n",
    "plt.ylim(0.5, 5.2)\n",
    "plt.vlines(4, 0, 6, colors='k', linestyles='dashed')\n",
    "plt.ylabel(r'DN/$10^4$')\n",
    "plt.text(29, 0.8, 'x=700, y=850', fontsize=15)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(group_id, ramp2, 'ok', label='ramp data')\n",
    "plt.plot(group_id[3:5], ramp2[3:5], 'sC3', markerfacecolor='none', ms=10, label='masked')\n",
    "plt.legend(loc=4)\n",
    "plt.vlines(4, 0, 6, colors='k', linestyles='dashed')\n",
    "\n",
    "plt.ylim(0.5, 5.2)\n",
    "plt.ylabel(r'DN/$10^4$')\n",
    "plt.xlabel('group')\n",
    "plt.text(29, 1.8, 'x=700, y=150', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf4d15",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "output_dir = '../data/jump/'\n",
    "\n",
    "# Find the raw-data path for the two jump F2100W exposures \n",
    "input_dirs = sorted( glob(\"../data/raw/jw01345/mastDownload/JWST/jw0134500*001_14101*mirimage/\") )\n",
    "\n",
    "## Stage 1\n",
    "# Iterate over each input directory \n",
    "for input_dir in input_dirs:\n",
    "    # Search for raw files in the directory\n",
    "    raw_files = sorted(glob(input_dir+'*_uncal.fits'))\n",
    "    # Iterate over each raw file\n",
    "    for file in raw_files:\n",
    "        # Set up the pipeline\n",
    "        det1 = Detector1Pipeline()\n",
    "        det1.save_results = False\n",
    "        # Use parallel, can be 'none', 'quarter', 'half', and 'all', or a fractional number\n",
    "        det1.ramp_fit.maximum_cores = 'quarter'\n",
    "        det1.jump.maximum_cores = det1.ramp_fit.maximum_cores\n",
    "        # Run till jump_fit \n",
    "        # (i.e., skip the last two steps)\n",
    "        det1.ramp_fit.skip = True\n",
    "        det1.gain_scale.skip = True\n",
    "        res0 = det1.run(file)\n",
    "        \n",
    "        # Do the correction\n",
    "        res = correct_jump(res0)\n",
    "        #res = res0.copy()\n",
    "        \n",
    "        # Run ramp_fit\n",
    "        # (gain_scale is skipped by default)\n",
    "        det1.ramp_fit.skip = False\n",
    "        det1.ramp_fit.save_results = True\n",
    "        res = det1.ramp_fit(res)\n",
    "        \n",
    "# Rename and move the products \n",
    "rate_fnames = glob('*0_ramp_fit.fits')\n",
    "rateints_fnames = glob('*1_ramp_fit.fits')\n",
    "for rate_fname, rateints_fname in zip(rate_fnames, rateints_fnames):\n",
    "    os.system('mv ' + rate_fname     + ' ' + output_dir + \\\n",
    "               rate_fname.replace('0_ramp_fit', 'rate'))\n",
    "    os.system('mv ' + rateints_fname + ' ' + output_dir + \\\n",
    "               rateints_fname.replace('1_ramp_fit', 'rateints'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18b160",
   "metadata": {},
   "source": [
    "# Search for stage-1 result files\n",
    "stage1_files = sorted(glob(output_dir+'*_rate.fits'))\n",
    "flt_groups, flt_vals = group_imgs(stage1_files, 'band')\n",
    "# Iterate over each file\n",
    "for files, flt_val in zip(flt_groups, flt_vals):\n",
    "    for file in files:\n",
    "        # Set up the pipeline\n",
    "        img2 = Image2Pipeline()\n",
    "        img2.save_results = True\n",
    "        # Run the pipeline\n",
    "        img2.run(file)\n",
    "        # Move the result to output directory \n",
    "        os.system('mv *.fits ' + output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94c826",
   "metadata": {},
   "source": [
    "# Subtract background for stage-2 result \n",
    "# Using Casey's algorithm\n",
    "# Search for original stage-2 result files\n",
    "stage2_files = sorted(glob(output_dir+'*_cal.fits'))\n",
    "# Remove strips or global background\n",
    "Nc = min(multiprocessing.cpu_count()//4, len(stage2_files))\n",
    "with multiprocessing.Pool(processes=Nc) as pool_obj:\n",
    "    pool_obj.map(rm_strip, stage2_files)\n",
    "    \n",
    "# Remove background \n",
    "stage2_files = sorted(glob(output_dir+'*stprm.fits'))\n",
    "# Extract filter info for each file \n",
    "flt_groups, flt_vals = group_imgs(stage2_files, 'band')\n",
    "# subtract the background group by group\n",
    "for flt, files in zip(flt_vals, flt_groups):\n",
    "    print(flt+':')\n",
    "    # Define the parallel function\n",
    "    def do_f0(f0): \n",
    "        super_bg(f0, files)\n",
    "        return\n",
    "    # Decide number of cores to use\n",
    "    Nc = min(multiprocessing.cpu_count()//4, len(files))\n",
    "    # Run\n",
    "    with multiprocessing.Pool(processes=Nc) as pool_obj:\n",
    "        pool_obj.map(do_f0, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6ba30",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Stage 3\n",
    "\n",
    "# Search for stage-2 result files\n",
    "stage2_files = sorted(glob(output_dir+'*14101*_stprm_bgsub.fits'))\n",
    "name = 'bad_uncorrected_'\n",
    "#name = 'bad_corrected_'\n",
    "#stage2_files = sorted(glob(output_dir+'*12101*_stprm_bgsub.fits'))\n",
    "#name = 'good_'\n",
    "\n",
    "# Group files by field_filter\n",
    "ff_groups, ff_vals = group_imgs(stage2_files, 'field_band')\n",
    "\n",
    "# Define the function to parallel\n",
    "def do_ff(ff_idx):\n",
    "    group = ff_groups[ff_idx]\n",
    "    \n",
    "    # Create an association \n",
    "    asn = dict( asn_from_list.asn_from_list(group, rule=Asn_Lv3Image, \n",
    "                product_name='l3_results', asn_type=\"image3\") )\n",
    "    # Initialize stage 3 pipeline\n",
    "    # In the following, we use \".copy()\" as input to avoid the input being changed \n",
    "    img3 = Image3Pipeline()\n",
    "    # Load the data models\n",
    "    dms = datamodels.open(asn)\n",
    " \n",
    "    cr_sigclip=1\n",
    "    tkreg_res = run_tkreg(dms, img3, cr_sigclip=cr_sigclip, run2=True)\n",
    "\n",
    "    # match bkg\n",
    "    img3.skymatch.subtract = True\n",
    "    img3.skymatch.skymethod = 'local'\n",
    "    res = img3.skymatch.run(tkreg_res.copy())\n",
    "\n",
    "    # Reject cosmic-ray \n",
    "    res = img3.outlier_detection.run(res)\n",
    "    \n",
    "    # First resample to get the coordinate transformation\n",
    "    img3.resample.rotation = -49.7\n",
    "    img3.resample.pixel_scale = 0.09\n",
    "    img3.resample.output_shape = 1500, 1500\n",
    "    temp_res = img3.resample(res.copy())    \n",
    "    \n",
    "    # Calculate the trangential point coordinates\n",
    "    tg_ra, tg_dec = 214.825, 52.825\n",
    "    tg_x, tg_y = temp_res.get_fits_wcs().world_to_pixel( SkyCoord(tg_ra*u.degree, tg_dec*u.degree, frame='icrs') )\n",
    "    # Round to 0.5 to align with HST \n",
    "    tg_x, tg_y = np.round(tg_x)+60.5, np.round(tg_y)+60.5\n",
    "    # Resample again to get the TG point correct\n",
    "    img3.resample.crval = tg_ra, tg_dec\n",
    "    img3.resample.crpix = tg_x, tg_y\n",
    "    res = img3.resample(res)\n",
    "    # Output \n",
    "    output_fname = output_dir + name + ff_vals[ff_idx].lower() + '_i2d.fits'\n",
    "    res.save(output_fname)\n",
    "\n",
    "# Decide number of cores to use\n",
    "Nc = min(multiprocessing.cpu_count()//4, len(ff_vals))\n",
    "# Run\n",
    "with multiprocessing.Pool(processes=Nc) as pool_obj:\n",
    "    pool_obj.map(do_ff, range(len(ff_vals)))\n",
    "\n",
    "# Clean up temporary files from intermediate steps\n",
    "os.system('rm *.fits')\n",
    "\n",
    "# Produce the RMS map\n",
    "stage3_files = sorted(glob(output_dir + '*i2d.fits'))\n",
    "# Array to save RMS scaling factor\n",
    "rms_facs = []\n",
    "for file in stage3_files:\n",
    "    rms_facs.append( write_rms_map(file, input_type='wht') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883f57e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "from astropy.table import Table\n",
    "from astropy.wcs import WCS\n",
    "from astropy.wcs.utils import proj_plane_pixel_scales\n",
    "%run -i '../../tools/astrometry/cut_img.ipynb'\n",
    "\n",
    "# Re-write the science images \n",
    "\n",
    "# Read science images \n",
    "sci_files = glob.glob('../data/jump/*f2100w*i2d.fits')\n",
    "sci_files.extend( glob.glob('../data/jump/uncorrected/*f2100w*i2d.fits') )\n",
    "\n",
    "# Itereate over each file\n",
    "for sci_file in sci_files:\n",
    "    # Load the multi-extension fits file\n",
    "    with fits.open(sci_file) as hdu:\n",
    "        sci_hdu = hdu[1]\n",
    "        cdelt = sci_hdu.header['CDELT1']\n",
    "        sci_hdu.header['CD1_1']=sci_hdu.header['PC1_1'] *cdelt\n",
    "        sci_hdu.header['CD1_2']=sci_hdu.header['PC1_2'] *cdelt\n",
    "        sci_hdu.header['CD2_1']=sci_hdu.header['PC2_1'] *cdelt\n",
    "        sci_hdu.header['CD2_2']=sci_hdu.header['PC2_2'] *cdelt\n",
    "        sci_hdu.header.remove('PC1_1')\n",
    "        sci_hdu.header.remove('PC1_2')\n",
    "        sci_hdu.header.remove('PC2_1')\n",
    "        sci_hdu.header.remove('PC2_2')   \n",
    "        sci_hdu.header.remove('CDELT1')\n",
    "        sci_hdu.header.remove('CDELT2')\n",
    "        \n",
    "        # Dump the science extension to an image file\n",
    "        # Note: can't use \"sci_hdu.write\", b/c that will place the image \n",
    "        # at the fits extension, which is incompatible with TPHOT  \n",
    "        fits.writeto( '../data/tphot/images/' + \\\n",
    "                      sci_file.split('/')[-1].replace('i2d', 'sci'),\n",
    "                      sci_hdu.data, header=sci_hdu.header, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470cff0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "input_dirs = sorted( glob(\"../data/raw/jw01345/mastDownload/JWST/jw01345001001_14101_00003_mirimage/\") )\n",
    "\n",
    "## Stage 1\n",
    "# Iterate over each input directory \n",
    "for input_dir in input_dirs:\n",
    "    # Search for raw files in the directory\n",
    "    raw_files = sorted(glob(input_dir+'*_uncal.fits'))\n",
    "    # Iterate over each raw file\n",
    "    for file in raw_files:\n",
    "        # Set up the pipeline\n",
    "        det1 = Detector1Pipeline()\n",
    "        det1.save_results = False\n",
    "        # Use parallel, can be 'none', 'quarter', 'half', and 'all', or a fractional number\n",
    "        det1.ramp_fit.maximum_cores = 'quarter'\n",
    "        det1.jump.maximum_cores = det1.ramp_fit.maximum_cores\n",
    "        # Run till jump_fit \n",
    "        # (i.e., skip the last two steps)\n",
    "        det1.ramp_fit.skip = True\n",
    "        det1.gain_scale.skip = True\n",
    "        \n",
    "        det1.jump.save_results = True\n",
    "        res0 = det1.run(file)\n",
    "\n",
    "res = correct_jump(res0.copy())\n",
    "res.save('../data/jump/jw01345001001_14101_00003_mirimage_jump_corrected.fits')\n",
    "\n",
    "# Run ramp_fit\n",
    "# (gain_scale is skipped by default)\n",
    "det1.ramp_fit.skip = False\n",
    "det1.ramp_fit.save_results = True\n",
    "res = det1.ramp_fit(res)\n",
    "\n",
    "# Rename and move the products \n",
    "rate_fnames = glob('*0_ramp_fit.fits')\n",
    "rateints_fnames = glob('*1_ramp_fit.fits')\n",
    "for rate_fname, rateints_fname in zip(rate_fnames, rateints_fnames):\n",
    "    os.system('mv ' + rate_fname     + ' ' + output_dir + \\\n",
    "               rate_fname.replace('0_ramp_fit', 'rate'))\n",
    "    os.system('mv ' + rateints_fname + ' ' + output_dir + \\\n",
    "               rateints_fname.replace('1_ramp_fit', 'rateints'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwst_pipeline",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
